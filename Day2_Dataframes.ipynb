{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 2: Exploring tabulated data with Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pandas is the best-in-slot tabulated data package in Python. Due to its immense popularity there is a vast variety of functionality build into it and a wealth of resources available on working with Pandas online. To start things off we will import pandas and give it the nickname 'pd' (Once again a convention within the Python community)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nORy44FxU3Tu"
      },
      "outputs": [],
      "source": [
        "# We import pandas and abbreviate it to pd\n",
        "import pandas as pd\n",
        "# We are also going to need matplotlib to plot our data\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So first let us open our dear friend: the Microsoft Excel file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYRLUVkiU3hs",
        "outputId": "886d2263-cc23-45f3-9c51-bbcce04ba0d9"
      },
      "outputs": [],
      "source": [
        "# We can point to an Excel file by providing the path to the file\n",
        "excel_file = pd.ExcelFile(\"./data/Tabulated/CTS_Database.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a few things to note from our previous cell of code:\n",
        "- First: We declare the Excel file like we would a variable. We need an object to contain the data of the Excel file.\n",
        "- Secondly: We use the ExcelFile method of pandas to open Excel Files.\n",
        "- Third: We use the file path relative to the script to locate the file. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see which sheets are in the Excel file by calling the sheet_names property of Excel File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# And see the name of each sheet by\n",
        "excel_file.sheet_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's inspect the Time Waterlevel sheet. A sheet can of course contain a lot of data. So if we just want to view the top few records of a sheet we do so with the .head() method.\n",
        "- You can also specify a number within the parenthesis of the method to show that many record (e.g. .head(10) to show the first 10 records.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kgiqD2woVSLR",
        "outputId": "2204a76b-7833-4a1e-f696-d61588c8b5a2"
      },
      "outputs": [],
      "source": [
        "# We can open a specific sheet as follows\n",
        "df_wl = excel_file.parse(\"Time WL\")\n",
        "# And take a peak at the files contents\n",
        "df_wl.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For some more information regarding the number of records and the different types of columns present in the sheet we can use the .info() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq6SdG0XWORL",
        "outputId": "fc442b70-4c5b-40a5-ba77-bbbcd658f7dd"
      },
      "outputs": [],
      "source": [
        "# We can get some information\n",
        "df_wl.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above we can see that there are four columns in the sheet, with a total of 690 records. The .info() method will also tell us the data type of each column.\n",
        "\n",
        "Now let us get a list of the unique SiteNames present in the sheet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_wl[\"SiteName\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the above code we can see that columns are indexed the same way we would a list or dictionary, except instead of an index we provide the name of the column. Then once we have the data for just that column we then proceed to call the .unique() method to get all unique values in that column.\n",
        "\n",
        "Using the above list, let's take a look at the last unique SiteName: NH3. In order to get all the records matching the SiteName NH3 we will use the .loc method of the dataframe. When using the .loc method we do provide parenthesis, but instead provide square brackets to indicate the range we are interested in. Then within the square brackets we use a statement that effectively says:\n",
        "\n",
        "From our dataframe, get me each record whose SiteName column is equal to NH3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2QohCjYrVcU0",
        "outputId": "2067ae0f-daec-494d-b114-acd0fa56fae3"
      },
      "outputs": [],
      "source": [
        "# If we only want to look at one borehole's data\n",
        "df_NH3 = df_wl.loc[df_wl[\"SiteName\"] == \"NH3\"]\n",
        "df_NH3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since we are now looking at only NH3 in the new df_NH3 dataframe, we don't need that SiteName column anymore. So let us make the DateTimeMeas our new index column and get rid of the NH3 column. While we are at it we will also delete the Status column which doesn't contain any records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "uCXQvKrNW0Dc",
        "outputId": "33fec896-75d3-40db-99da-0d0429abf880"
      },
      "outputs": [],
      "source": [
        "# Let's make the DateTimeMeas the index of the table\n",
        "df_NH3.set_index(\"DateTimeMeas\", inplace=True)\n",
        "# And drop the status and sitename columns\n",
        "df_NH3 = df_NH3.drop(columns=[\"SiteName\", \"Status\"])\n",
        "df_NH3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we just have the DateTimeMeas and Waterlevel columns for NH3, let's get some basic statistics for the data using the .describe() method of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "A7KaBQ1yYShD",
        "outputId": "f4933287-051f-4bc7-88d5-e52921ebecb2"
      },
      "outputs": [],
      "source": [
        "# Get a description of our data\n",
        "df_NH3.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use these basic statistics to determine if any outliers are present and if we need to modify the data. In this case, we can see that we have 278 waterlevels, with an average waterlevel of approximately 12.6 meters below ground level. Both the maximum and minimum values appear valid. \n",
        "\n",
        "Now the best way to further explore the data would be to first plot a graph of the data. Pandas provides build-in methods to automatically plot data for us with matplotlib, let's take a look at the code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "HG8ZTjkRYZdW",
        "outputId": "400c110a-5b91-4a6e-da8a-7efa26ff0589"
      },
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "df_NH3.plot(\n",
        "    kind=\"line\",\n",
        "    color=\"red\",\n",
        "    xlabel=\"Date\",\n",
        "    ylabel=\"Water level (???)\",\n",
        "    title=\"Plot of NH3 water level\"\n",
        ")\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code to plot the figure looks a bit different when we use Pandas, let's break it down:\n",
        "- We call the .plot() method on the dataframe to plot it.\n",
        "- Additionally, we can define some parameters to customise the plot\n",
        "- With the \"kind\" property we define what kind of graph we would like.\n",
        "- The \"color\" property is self explanatory.\n",
        "- \"xlabel\" and \"ylabel\" is used to give title to the x- and y-axis respectively.\n",
        "- The \"title\" propoerty is also self explanatory.\n",
        "\n",
        "And voila, we have a graph. However, it is clear that something is wrong here. This data appears to be measuring the waterlevel from a certain datum located below the water level. In reality, this is data freshly extracted from a pressure transducer which has not been converted to meters below ground level yet. Let us correct that, the water level measured in NH3 when the pressure transducer was removed (the final reading) is 15.5 mbgl. Let's convert the data to the new datum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_HW_owGOhCY"
      },
      "outputs": [],
      "source": [
        "# First, let us get the last water level recording from the data\n",
        "last_record = df_NH3[\"Waterlevel\"].iloc[-1]\n",
        "print(last_record)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The process was similar to what we used before. First, we only grab the Waterlevel column from the dataset and then we get the last position by using the index locator (.iloc) method and the same indexing style we would use with a list. With this we get a water level of 10.40 meters above datum, which is then equivalent to our 15.5 mbgl measurement. This puts the installation depth of our logger at:\n",
        "\n",
        "15.5 + 10.4 = 25.9 mbgl\n",
        "\n",
        "Now we can change our data to mbgl by subtracting the recorded value from this number:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_NH3[\"CorrWaterlevel\"] = 25.9 - df_NH3[\"Waterlevel\"]\n",
        "df_NH3.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can plot our data again, but since our dataframe has two data columns now we will specify that it needs to only plot the corrected waterlevel column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the data\n",
        "df_NH3[\"CorrWaterlevel\"].plot(\n",
        "    kind=\"line\",\n",
        "    color=\"blue\",\n",
        "    xlabel=\"Date\",\n",
        "    ylabel=\"Water level (meters below ground level)\",\n",
        "    title=\"Plot of NH3 water level\"\n",
        ")\n",
        "# Since we are now working with mbgl, it would help visualisation to invert the y-axis\n",
        "plt.gca().invert_yaxis()\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that is exactly what we expected to see. Let us now move on to some common parts of data exploration and preparation. Anyone that has worked with real data knows how messy it can be. We will now explore some common workflows for handling data.\n",
        "\n",
        "# Data Quality Checks and Fixes\n",
        "\n",
        "First, let us open the BasicInfo sheet of the WISH database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We can open the basic info sheet as follows\n",
        "df_basic = excel_file.parse(\"BasicInf\")\n",
        "# And take a peak at the files contents\n",
        "df_basic.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let us look at how to handle a common problem: duplicates in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "complete_duplicates = df_basic[df_basic.duplicated() == True]\n",
        "complete_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can remove the complete duplicates by calling the .drop_duplicates() method of the dataframe and passing no arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The drop duplicates method will drop all records that are complete duplicates of another one\n",
        "df_basic = df_basic.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to find all duplicates just based on a specific column then we specify that column. The \"keep\" property is usually true by default. Here we set it to False so that it shows us the duplicates and the first instance of the same name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicate_names = df_basic[df_basic.duplicated('SiteName', keep=False) == True]\n",
        "duplicate_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's clear that the duplicates which are the second instance of the name has the same values, just at a lower accuracy than the first instances. So we will drop the second instances using the .drop_duplicates() method and passing the column that we would like to be checked. Note that 'keep' property is now kept at its default value, which will keep the first instance of a SiteName but remove and duplicates following it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_basic = df_basic.drop_duplicates('SiteName')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's look at some information regarding our dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_basic.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's drop the records where Xcoord or Ycoord is missing. But first, we'll make a dataframe listing those values so that we can send it to our client for feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_coords = df_basic[df_basic['Xcoord'].isna() | df_basic['Ycoord'].isna()]\n",
        "df_missing_coords.to_excel('.\\output\\missing_coordinates.xlsx')  # Show how to fix the warning\n",
        "df_missing_coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_basic = df_basic.dropna(subset=['Xcoord', 'Ycoord'])\n",
        "df_basic.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That problem is solved, let's look at some statistics of the dataaset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_basic.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the statistics we can see we have some coordinates which have been swapped around. Let's fix that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "swapped = df_basic[df_basic['Xcoord'] < -1000000]  # assuming Y will be a larger negative value than X\n",
        "swapped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we know which two records have the problems, we can swap their coordinates around:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_coord_UO21 = df_basic.loc[df_basic['SiteName'] == 'UO21']['Ycoord'].iloc[0]\n",
        "y_coord_UO21 = df_basic.loc[df_basic['SiteName'] == 'UO21']['Xcoord'].iloc[0]\n",
        "\n",
        "df_basic.loc[df_basic['SiteName'] == 'UO21', 'Xcoord'] = x_coord_UO21\n",
        "df_basic.loc[df_basic['SiteName'] == 'UO21', 'Ycoord'] = y_coord_UO21\n",
        "\n",
        "x_coord_UP15 = df_basic.loc[df_basic['SiteName'] == 'UP15']['Ycoord'].iloc[0]\n",
        "y_coord_UP15 = df_basic.loc[df_basic['SiteName'] == 'UP15']['Xcoord'].iloc[0]\n",
        "\n",
        "df_basic.loc[df_basic['SiteName'] == 'UP15', 'Xcoord'] = x_coord_UP15\n",
        "df_basic.loc[df_basic['SiteName'] == 'UP15', 'Ycoord'] = y_coord_UP15\n",
        "\n",
        "df_basic.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see we also have some problems in the Depth column. Let's look for unrealistic depth values and put them in a dataframe to send to our client:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_invalid_depth = df_basic[(df_basic['Depth'] < 0) | (df_basic['Depth'] > 500)]\n",
        "df_invalid_depth.to_excel(r'.\\output\\invalid_depth.xlsx')\n",
        "df_invalid_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that concludes our examples for DataFrames and Pandas. Now let's start practicing:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrame Exercise:\n",
        "\n",
        "Prepare and clean the borehole database provided in the file \"borehole_data.csv\". A few of the problems we discussed today is present in the database. With the help of the examples above prepare the database for use. The file is loaded for you in the cell below. Add as many cells as you wish to complete this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First we open the file into a pandas dataframe\n",
        "df_csv = pd.read_csv(\"./data/Tabulated/borehole_data.csv\")\n",
        "df_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start from here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
